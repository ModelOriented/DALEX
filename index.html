<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>moDel Agnostic Language for Exploration and eXplanation • DALEX</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="bootstrap-toc.css">
<script src="bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- dalexverse --><link href="dalexverse.css" rel="stylesheet">
<link href="dalexverse-2.css" rel="stylesheet">
<!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="moDel Agnostic Language for Exploration and eXplanation">
<meta property="og:description" content="Unverified black box model is the path to the failure. Opaqueness leads to distrust. 
  Distrust leads to ignoration. Ignoration leads to rejection. 
  DALEX package xrays any model and helps to explore and explain its behaviour.
  Machine Learning (ML) models are widely used and have various applications in classification 
  or regression. Models created with boosting, bagging, stacking or similar techniques are often
  used due to their high performance. But such black-box models usually lack of direct interpretability.
  DALEX package contains various methods that help to understand the link between input variables 
  and model output. Implemented methods help to explore model on the level of a single instance 
  as well as a level of the whole dataset.
  All model explainers are model agnostic and can be compared across different models.
  DALEX package is the cornerstone for DrWhy.AI universe of packages for visual model exploration.
  Find more details in (Biecek 2018) &lt;arXiv:1806.08915&gt;.">
<meta property="og:image" content="/logo.png">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- google analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-5650686-14"></script><script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'UA-5650686-14');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-home">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <div class="navbar-brand-container">
        <a class="navbar-brand" href="index.html">DALEX</a>
        <div class="info">
          <span class="partof">part of the <a href="https://github.com/ModelOriented/DrWhy">DrWhy.AI</a>
           developed by the <a href="https://mi2.mini.pw.edu.pl/">MI^2 DataLab</a> </span>
          <span class="version version-default">2.3.0.9000</span>
        </div>
      </div>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav navbar-right">
<li>
  <a href="index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/multilabel_classification.html">How to use multilabel classification and DALEX?</a>
    </li>
    <li>
      <a href="articles/vignette_titanic.html">Survival on the RMS Titanic</a>
    </li>
  </ul>
</li>
<li>
  <a href="news/index.html">Changelog</a>
</li>
        <li>
  <a href="https://github.com/ModelOriented/DALEX/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="contents col-md-9">
<div id="model-agnostic-language-for-exploration-and-explanation-" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#model-agnostic-language-for-exploration-and-explanation-" class="anchor"></a>moDel Agnostic Language for Exploration and eXplanation <img src="reference/figures/logo.png" align="right" width="150">
</h1></div>

<p><a href="https://github.com/ModelOriented/DALEX/actions?query=workflow%3APython-check"><img src="https://github.com/ModelOriented/DALEX/workflows/Python-check/badge.svg" alt="Python-check"></a> <a href="https://pypi.org/project/dalex/"><img src="https://img.shields.io/pypi/pyversions/dalex.svg" alt="Supported Python versions"></a> <a href="https://badge.fury.io/py/dalex"><img src="https://badge.fury.io/py/dalex.svg" alt="PyPI version"></a> <a href="https://pepy.tech/project/dalex"><img src="https://pepy.tech/badge/dalex" alt="Downloads"></a></p>
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p>Unverified black box model is the path to the failure. Opaqueness leads to distrust. Distrust leads to ignoration. Ignoration leads to rejection.</p>
<p>The <code>DALEX</code> package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. The main function <code><a href="reference/explain.default.html">explain()</a></code> creates a wrapper around a predictive model. Wrapped models may then be explored and compared with a collection of local and global explainers. Recent developents from the area of Interpretable Machine Learning/eXplainable Artificial Intelligence.</p>
<p>The philosophy behind <code>DALEX</code> explanations is described in the <a href="https://pbiecek.github.io/ema/">Explanatory Model Analysis</a> e-book. The <code>DALEX</code> package is a part of <a href="http://DrWhy.AI">DrWhy.AI</a> universe.</p>
<p>If you work with <code>scikit-learn</code>, <code>keras</code>, <code>H2O</code>, <code>tidymodels</code>, <code>xgboost</code>, <code>mlr</code> or <code>mlr3</code> in R, you may be interested in the <a href="https://github.com/ModelOriented/DALEXtra">DALEXtra</a> package, which is an extension of <code>DALEX</code> with easy to use <code>explain_*()</code> functions for models created in these libraries.</p>
<p><strong><a href="https://github.com/ModelOriented/DALEX/tree/master/python/dalex">Additional overview of the dalex Python package is available.</a></strong></p>
<p align="center">
<a href="https://pbiecek.github.io/ema/introduction.html#bookstructure"><img src="https://github.com/ModelOriented/DALEX/raw/master/misc/DALEXpiramide.png" width="800"></a>
</p>
</div>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>The <code>DALEX</code> <strong>R</strong> package can be installed from <a href="https://cran.r-project.org/package=DALEX">CRAN</a></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"DALEX"</span><span class="op">)</span></code></pre></div>
<p>The <code>dalex</code> <strong>Python</strong> package is available on <a href="https://pypi.org/project/dalex/">PyPI</a> and <a href="https://anaconda.org/conda-forge/dalex">conda-forge</a></p>
<pre class="console"><code>pip install dalex -U

conda install -c conda-forge dalex</code></pre>
</div>
<div id="learn-more" class="section level2">
<h2 class="hasAnchor">
<a href="#learn-more" class="anchor"></a>Learn more</h2>
<p>Machine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides.</p>
<p>In many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. <code>DALEX</code> is a set of tools that help to understand how complex models are working.</p>
<p align="center">
<a href="https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png"><img src="https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png" width="500"></a>
</p>
</div>
<div id="resources" class="section level2">
<h2 class="hasAnchor">
<a href="#resources" class="anchor"></a>Resources</h2>
<ul>
<li><a href="https://pbiecek.github.io/ema/">Gentle introduction to DALEX with examples in R and Python</a></li>
</ul>
<div id="r-package" class="section level3">
<h3 class="hasAnchor">
<a href="#r-package" class="anchor"></a>R package</h3>
<ul>
<li><a href="https://github.com/MI2DataLab/ResponsibleML-UseR2021">Introduction to Responsible Machine Learning @ useR! 2021</a></li>
<li>DALEX + mlr3 <a href="https://github.com/pbiecek/BioColl2021">@ BioColl 2021</a> &amp; <a href="https://github.com/pbiecek/Open-Forest-Training-2021/">@ Open-Forest-Training 2021</a>
</li>
<li>
<a href="https://github.com/pbiecek/XAIatERUM2020">Materials from Explanatory Model Analysis Workshop @ eRum 2020</a>, <a href="https://github.com/pbiecek/XAIatERUM2020/blob/master/Cheatsheet.pdf">cheatsheet</a>
</li>
<li>How to use DALEX with: <a href="https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_keras.html">keras</a>, <a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_parsnip.html">parsnip</a>, <a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html">caret</a>, <a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_mlr.html">mlr</a>, <a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_h2o.html">H2O</a>, <a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_xgboost.html">xgboost</a>
</li>
<li>
<a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/Multilanguages_comparision.html">Compare GBM models created in different languages</a>: gbm and CatBoost in R / gbm in h2o / gbm in Python</li>
<li><a href="https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEXverse%20and%20fraud%20detection.html">DALEX for fraud detection</a></li>
<li><a href="https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_teaching.html">DALEX for teaching</a></li>
<li><a href="https://medium.com/@ModelOriented/xai-in-the-jungle-of-competing-frameworks-for-machine-learning-fa6e96a99644">XAI in the jungle of competing frameworks for machine learning</a></li>
</ul>
</div>
<div id="python-package" class="section level3">
<h3 class="hasAnchor">
<a href="#python-package" class="anchor"></a>Python package</h3>
<ul>
<li>Introduction to the <code>dalex</code> package: <a href="https://dalex.drwhy.ai/python-dalex-titanic.html">Titanic: tutorial and examples</a>
</li>
<li>Key features explained: <a href="https://dalex.drwhy.ai/python-dalex-fifa.html">FIFA20: explain default vs tuned model with dalex</a>
</li>
<li>How to use dalex with: <a href="https://dalex.drwhy.ai/python-dalex-xgboost.html">xgboost</a>, <a href="https://dalex.drwhy.ai/python-dalex-tensorflow.html">tensorflow</a>
</li>
<li>More explanations: <a href="https://dalex.drwhy.ai/python-dalex-new.html">residuals, shap, lime</a>
</li>
<li>Introduction to the <a href="https://dalex.drwhy.ai/python-dalex-fairness.html">Fairness module in dalex</a>
</li>
<li>Introduction to the <a href="https://dalex.drwhy.ai/python-dalex-arena.html">Arena: interactive dashboard for model exploration</a>
</li>
<li>Code in the form of <a href="https://github.com/ModelOriented/DALEX-docs/tree/master/jupyter-notebooks">jupyter notebook</a>
</li>
<li>Changelog: <a href="https://github.com/ModelOriented/DALEX/blob/master/python/dalex/NEWS.md">NEWS</a>
</li>
</ul>
</div>
<div id="talks-about-dalex" class="section level3">
<h3 class="hasAnchor">
<a href="#talks-about-dalex" class="anchor"></a>Talks about DALEX</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=9WWn5ew8D8o">Talk with your model! at USeR 2020</a></li>
<li><a href="https://github.com/pbiecek/Talks/blob/master/2018/DALEX_at_NTU_2018.pdf">Talk about DALEX at Complexity Institute / NTU February 2018</a></li>
<li><a href="https://github.com/pbiecek/Talks/blob/master/2018/SER_DALEX.pdf">Talk about DALEX at SER / WTU April 2018</a></li>
<li><a href="https://github.com/STWUR/eRementarz-29-05-2018">Talk about DALEX at STWUR May 2018 (in Polish)</a></li>
<li><a href="https://github.com/pbiecek/Talks/blob/master/2018/DALEX_BayArea.pdf">Talk about DALEX at BayArea 2018</a></li>
<li><a href="https://github.com/pbiecek/Talks/blob/master/2018/DALEX_PyDataWarsaw2018.pdf">Talk about DALEX at PyData Warsaw 2018</a></li>
</ul>
</div>
</div>
<div id="citation" class="section level2">
<h2 class="hasAnchor">
<a href="#citation" class="anchor"></a>Citation</h2>
<p>If you use <code>DALEX</code> in R or <code>dalex</code> in Python, please cite our JMLR papers:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb3-1"><a href="#cb3-1"></a>@article{JMLR:v19:18-416,</span>
<span id="cb3-2"><a href="#cb3-2"></a>  author  = {Przemyslaw Biecek},</span>
<span id="cb3-3"><a href="#cb3-3"></a>  title   = {DALEX: Explainers for Complex Predictive Models in R},</span>
<span id="cb3-4"><a href="#cb3-4"></a>  journal = {Journal of Machine Learning Research},</span>
<span id="cb3-5"><a href="#cb3-5"></a>  year    = {2018},</span>
<span id="cb3-6"><a href="#cb3-6"></a>  volume  = {19},</span>
<span id="cb3-7"><a href="#cb3-7"></a>  number  = {84},</span>
<span id="cb3-8"><a href="#cb3-8"></a>  pages   = {1-5},</span>
<span id="cb3-9"><a href="#cb3-9"></a>  url     = {http://jmlr.org/papers/v19/18-416.html}</span>
<span id="cb3-10"><a href="#cb3-10"></a>}</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a>@article{JMLR:v22:20-1473,</span>
<span id="cb3-13"><a href="#cb3-13"></a>  author  = {Hubert Baniecki and</span>
<span id="cb3-14"><a href="#cb3-14"></a>             Wojciech Kretowicz and</span>
<span id="cb3-15"><a href="#cb3-15"></a>             Piotr Piatyszek and </span>
<span id="cb3-16"><a href="#cb3-16"></a>             Jakub Wisniewski and </span>
<span id="cb3-17"><a href="#cb3-17"></a>             Przemyslaw Biecek},</span>
<span id="cb3-18"><a href="#cb3-18"></a>  title   = {dalex: Responsible Machine Learning </span>
<span id="cb3-19"><a href="#cb3-19"></a>             with Interactive Explainability and Fairness in Python},</span>
<span id="cb3-20"><a href="#cb3-20"></a>  journal = {Journal of Machine Learning Research},</span>
<span id="cb3-21"><a href="#cb3-21"></a>  year    = {2021},</span>
<span id="cb3-22"><a href="#cb3-22"></a>  volume  = {22},</span>
<span id="cb3-23"><a href="#cb3-23"></a>  number  = {214},</span>
<span id="cb3-24"><a href="#cb3-24"></a>  pages   = {1-7},</span>
<span id="cb3-25"><a href="#cb3-25"></a>  url     = {http://jmlr.org/papers/v22/20-1473.html}</span>
<span id="cb3-26"><a href="#cb3-26"></a>}</span></code></pre></div>
</div>
<div id="why" class="section level2">
<h2 class="hasAnchor">
<a href="#why" class="anchor"></a>Why</h2>
<p>76 years ago Isaac Asimov devised <a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Three Laws of Robotics</a>: 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around <a href="https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence">Ethics of AI</a>. Today’s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov’s ethics.</p>
<p>Today we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See <em>Weapons of Math Destruction</em> by Cathy O’Neil for an excellent overview of potential problems.</p>
<p>It’s clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill.</p>
<ul>
<li>
<strong>Prediction’s justifications</strong>. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction.</li>
<li>
<strong>Prediction’s speculations</strong>. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios.</li>
<li>
<strong>Prediction’s validations</strong> For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction.</li>
</ul>
<p>There are two ways to comply with these requirements. One is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance. The other way is to use approximated explainers – techniques that find only approximated answers, but work for any black box model. Here we present such techniques.</p>
</div>
<div id="acknowledgments" class="section level2">
<h2 class="hasAnchor">
<a href="#acknowledgments" class="anchor"></a>Acknowledgments</h2>
<p>Work on this package was financially supported by the <code>NCN Opus grant 2016/21/B/ST6/02176</code> and <code>NCN Opus grant 2017/27/B/ST6/0130</code>.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <div class="links">
<h2>Links</h2>
<ul class="list-unstyled">
<li>Download from CRAN at <br><a href="https://cloud.r-project.org/package=DALEX">https://​cloud.r-project.org/​package=DALEX</a>
</li>
<li>Browse source code at <br><a href="https://github.com/ModelOriented/DALEX/">https://​github.com/​ModelOriented/​DALEX/​</a>
</li>
<li>Report a bug at <br><a href="https://github.com/ModelOriented/DALEX/issues">https://​github.com/​ModelOriented/​DALEX/​issues</a>
</li>
</ul>
</div>
<div class="license">
<h2>License</h2>
<ul class="list-unstyled">
<li>GPL</li>
</ul>
</div>
<div class="community">
<h2>Community</h2>
<ul class="list-unstyled">
<li><a href="CONTRIBUTING.html">Contributing guide</a></li>
</ul>
</div>
<div class="citation">
<h2>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html">Citing DALEX</a></li>
</ul>
</div>
<div class="developers">
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Przemyslaw Biecek <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0001-8423-1823" target="orcid.widget" aria-label="ORCID"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
<li>Szymon Maksymiuk <br><small class="roles"> Author </small> <a href="https://orcid.org/0000-0002-3120-1601" target="orcid.widget" aria-label="ORCID"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
<li>Hubert Baniecki <br><small class="roles"> Author </small> <a href="https://orcid.org/0000-0001-6661-5364" target="orcid.widget" aria-label="ORCID"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

  <div class="dev-status">
<h2>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/ModelOriented/DALEX/actions?query=workflow%3AR-CMD-check"><img src="https://github.com/ModelOriented/DALEX/workflows/R-CMD-check/badge.svg" alt="R build status"></a></li>
<li><a href="https://codecov.io/github/ModelOriented/DALEX?branch=master"><img src="https://img.shields.io/codecov/c/github/ModelOriented/DALEX/master.svg" alt="Coverage Status"></a></li>
<li><a href="https://cran.r-project.org/package=DALEX"><img src="http://www.r-pkg.org/badges/version/DALEX" alt="CRAN_Status_Badge"></a></li>
<li><a href="http://cranlogs.r-pkg.org/badges/grand-total/DALEX"><img src="http://cranlogs.r-pkg.org/badges/grand-total/DALEX?color=orange" alt="Total Downloads"></a></li>
<li><a href="http://drwhy.ai/#BackBone"><img src="https://img.shields.io/badge/DrWhy-BackBone-373589" alt="DrWhy-eXtrAI"></a></li>
</ul>
</div>
</div>
</div>


      <footer><div class="copyright">
  <p>Developed by Przemyslaw Biecek, Szymon Maksymiuk, Hubert Baniecki.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
